# 1. Machine Learning Overview and Recap. of Math

## Theoretical vs. Applied Machine Learning

### Classification

- Ex) 소비자의 수입과 나이로 소비 유형 예측

  <img src="C:\Users\KJH\AppData\Roaming\Typora\typora-user-images\image-20200321013757615.png" alt="image-20200321013757615" style="zoom:80%;" />

  - 문제 조건
    1. 각 소비자는 '수입'과 '나이'라는 2가지 feature를 가짐
    2. 위의 표본들이 주어졌을 때, classification algorithm을 학습시켜라!
       - **두 class를 split하는 가장 적절한 boundary를 찾아야 함! => "Classification"**
  - Simple rule-based classifer를 설계할 수 있음
    - 초록색 원(70k <= income <= 100k && 30 <= age <= 45)일 때 premium
    - 그러면 빨간 별의 새로운 소비자 데이터가 등장했을 때 이 소비자가 premium purchase를 할 것이라고 predict할 수 있음

### Theoretical vs. Applied Machine Learning

- Theoretical Machine Learning
  - 더 적절한 boundary(hyperplane)를 발견하는 **method를 설계**하는 단계
    - 다양한 feature가 있을 수 있기 때문에(high-dimensional vector space) 유용한 feature를 통해 boundary를 설계하는 것은 매우 어려운 작업!
  - 고도의 수학(Linear algebra, mathematical optimization) 필요
- Applied Machine Learning(= Feature Engineering)
  - Feature engineering(데이터의 변수를 통해 feature(Ex. age, income)를 설정하는 것) 후 최첨단 classifier를 활용하는 단계
  - 고도의 domain knowledge???와 다른 종류의 수학(game theory model) 필요

### Feature Engineering

Ex) Data-driven Airline Profit Maximization (Feature Engineering이 많이 들어간 연구)

- 항공사에서 4개의 노선을 운항할 때, 제한된 예산(운항에 들어가는 비용)을 가장 최적으로 할당하여 수익을 최대화 하는 방법 구하기(flight frequency를 조정하여)
- 2가지 필요한 알고리즘
  - profit-frequency 간의 관계를 정확히 **예측(forecasting)**
    - frequency를 각각 배정했을 때 output으로 profit이 계산되도록
  - 위의 예측을 기반으로 **가장 적합하게 할당(optimal allocation)**

#### Forecasting

- Market share(시장 점유율)에 대한 예측(Regression)

  - Regression: 우리에게 주어진 feature로 market share를 예측하는 것
    - 여기서 feature는 ticket price, frequency, delay time, delay ratio, ...
  - 각 항공사에 대해 이 feature를 정의???

- Feature Engineering

  1. 각 항공사를 포함하는 zero-sum game theory model 설계	

     - LA->NY로 가는 초기 ticket price: Delta(\$190), AA(\$180), United($200)
     - 각 항공사는 경쟁사의 가격을 계속 모니터링하며 가격을 조정하려 할 것이다.

  2. 각 항공사가 자신의 equilibrium ticket price(균형 가격)를 찾는다.

     - equilibrium price: 수요와 공급이 일치할 때 성립되는 가격

  3. **이 "equilibrium ticket price와 suggested ticket price(시장 가격)의 차의 절대값"을 새로운 feature로 추가한다. (Feature Engineering)**

     - 시장가격이 더 높으면 상대적으로 비싸다고 생각되므로 시장 점유율이 내려갈 것이고, 시장가격이 더 낮으면 상대적으로 싸다고 생각되므로 시장 점유율이 올라갈 것을 예측할 수 있음

     - 이렇게 새로 추가한 feature는 데이터베이스에서 따온 다른 feature들보다 market share를 predict 하는 데 있어서 더욱 효과적일 것이다.

       ![image-20200321163845744](C:\Users\KJH\AppData\Roaming\Typora\typora-user-images\image-20200321163845744.png)

       - 데이터베이스에서 다운로드받은 basic feature만을 사용하는 것은 누구나 할 수 있음, 그러나 equilibrium ticket price를 활용하는 feature engineering은 이와 다름!
         - 그래서 더 어렵기도 함

#### Profit Maximization

이부분 잘 모르겠다...???

![image-20200321172538800](C:\Users\KJH\AppData\Roaming\Typora\typora-user-images\image-20200321172538800.png)

- $f_i$: route $i$에 대한 flight frequency
- $N_i(f_i)$: predicted market share(=profit)
  - $N_i$: regression model trained for route $i$ 
  - $\underset{R_i \in R}\sum N_i(f_i)$: 모든 route에 대해 예측한 market share를 모두 합한 값
    - 이 값이 최대가 되는 $f_i$를 구해야 한다!
    - 조건: $0\le f_i\le f_i^{max}, f_i\in\N, \forall R_i\in R,\underset{R_i \in R}\sum C_if_i\le b$ (지정된 flight frequency의 쌍을 이용해 발생한 총 비용이 budget을 넘지 않아야 함)
- 이렇게 구하는 방법을 prediction-driven optimization 이라고 한다.
  - prediction 작업($N_i$를 구하는 작업)을 통해 최적의 $f_i$의 쌍을 찾는 것???
  - Prediction: Machine learning techniques
  - Decision making: optimization
  - 이 두가지가 query를 통해 interact한다.
  - Prediction(ML) 자체만으로는 의미가 없을 수 있음
    - Decision making을 하기 위한 starting point가 된다.

Feature engineering은 최악의 경우 노력에 비해 아무 성과도 얻지 못 할 수도 있다..

### Artificial Intelligence

- AI가 포함하는 것
  - Reasoning, problem solving
    - Step-by-step reasoning in logical deductions(추론)
  - Knowledge representation
    - Expert knowledge in a narrow domain
    - Ex) IBM Watson
  - Planning, decision making
    - Game theory, optimization
    - Agent-based model
  - Learning
    - Machine learning (우리가 배우는 부분)
  - General(Multi-modal) intelligence

- AI $\supset$ ML $\supset$ DL
  - AI: Any technique that enables computers to <u>mimic human behavior</u>
  - ML: Ability to <u>learn without explicitly being programmed</u>
  - DL: Extract patterns from data <u>using neural networks</u>

## Recap. of Linear Algebra

### Several Types of Mathematical Objects

- Scalar: 숫자 1개(실수)

- Vector: 배열.  Ex) [1, 1.5, 2.278] $\in R^3$ (해당 vector는 3-dimensional vector)

  - d-dimensional vector: d차원 좌표계에서의 점

  ![image-20200323005031502](C:\Users\KJH\AppData\Roaming\Typora\typora-user-images\image-20200323005031502.png)

  - Basis vector
    - 3-dimensional space일 때, $b_x = (1, 0, 0), b_y = (0, 1, 0), b_z = (0, 0, 1)$ 3개의 벡터가 basis vector가 된다.
  - Linear combination
    - "모든 벡터는 basis vector들의 linear combination으로 표현할 수 있다"
    - 마찬가지로 3-dimensional space일 때, $v=[a, b, c]$이면 $v=a\cdot b_x + b\cdot b_y + c\cdot b_z$

- Matrix: 2차원의 배열

  - Diagonal matrix: 대각선의 원소만 값을 가지고, 나머지 원소는 전부 0인 경우

- Tensor: 3차원 이상의 배열

### Multiplying Matrices and Vectors

- Matrix Product
  - $A$가 **m**\*n matrix이고, $B$가 n\***p** matrix 일 때, $AB$는 **m**\***p** matrix가 된다.
  - Distributive(분배 법칙을 따름). $A(B+C) = AB + AB$
  - Associative(결합 법칙을 따름). $A(BC) = (AB)C$
  - Not commutative(교환 법칙은 따르지 않음). $AB \neq BA$

- Element-wise Product (= Hadamard Product)

  - $A$가 **m\*n** matrix이고, $B$가 **m\*n** matrix 일 때, $A\odot B$는 **m\*n** matrix가 된다.

    - 곱하는 두 행렬의 크기가 같아야 함

  - 결과의 각 원소는 두 피연산자의 각 위치의 원소를 곱한 값을 가진다.

    ![image-20200323011811122](C:\Users\KJH\AppData\Roaming\Typora\typora-user-images\image-20200323011811122.png)

- Transpose

  - $A^\top$는 $A$의 row와 column을 바꾼 matrix
  - $(AB)^\top = B^\top A^\top$

### Linear dependence and Span

- Linear Independence
  - vector의 집합 $\{v_1, v_2, ..., v_n\}$이 있을 때, 모든 가능한 $a_j$에 대하여 $v_i \neq \underset{j\neq i}\sum a_jv_j$를 만족한다면 해당 집합은 linearly independent!
    - 하나의 vector가 집합의 나머지 vector들로 표현되지 않아야 하는 것!
  - 가장 trivial한 예시: basis vector
- Square Matrix
  - row와 column의 크기가 같은 matrix
  - <u>Non-singular</u> square matrix: 각 row vector가 linearly independent한 square matrix

### Identity and Inverse Matrices

- Identity Matrix
  - $I = \begin{pmatrix} 1&0&0\\0&1&0\\0&0&1\end{pmatrix}$
  - $AA^{-1} = I$

#### The Rank of a matrix

#### Linear Transformation

#### Eigenvector and eigenvalue

